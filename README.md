# LLMs as Rankers

- Large Language Models (LLMs) have emerged as powerful tools for document ranking tasks, showcasing their remarkable ability to understand and rank documents without any task-specific training.
- Ranking strategies for utilizing LLMs in ranking tasks can be categorized into four main approaches: Pointwise, Pairwise, Listwise, and Setwise Ranking.
- These methods utilize different prompting techniques to instruct the LLM to output relevance estimations for candidate documents, given a user query and the documents as input.

# Pointwise Rankers

- Pointwise rankers estimate the relevance of each document independently, without considering other documents.
- The reranker takes both the query and a candidate document to directly generate a relevance score. These independent scores assigned to each document are then used to reorder the candidate set.
- The relevance score is calculated based on relevant the document is to the query or how likely the query can be generated from the document.
- We performed a detailed analysis of Retrieval-Augmented Generation (RAG) pipelines for Dense and Hybrid Retrieval, employing different types of reranking and Reciprocal Rank Fusion (RRF) techniques.
- The effectiveness of the Diversity Ranker, Lost In The Middle Ranker, Similarity Ranker, and RRF techniques was evaluated.
- For Dense retrieval, `INSTRUCTOR-XL` and `all-mpnet-base-v2` models were employed. BM25 retrieval was used for Sparse retrieval in the Hybrid pipelines. The `bge-reranker-large` model was used in the Similarity Ranker, and `ms-marco-MiniLM-L-12-v2` for the Diversity Ranker.

## Performance Evaluation of Rankers and RRF Techniques for Retrieval Pipelines

**Paper:** [Performance Evaluation of Rankers and RRF Techniques for Retrieval Pipelines](paper/rankers_rrf.pdf)

In the intricate world of LFQA and RAG, making the most of the LLM’s context window is paramount. Any wasted space or repetitive content limits the depth and breadth of the answers we can extract and generate. It’s a delicate balancing act to lay out the content of the context window appropriately.

With the addition of three rankers, viz., Diversity Ranker, Lost In The Middle Ranker, Similarity Rankers and RRF techniques, we aim to address these challenges and improve the answers generated by the LFQA/RAG pipelines. We have done a comparative study of adding different combinations of rankers in a Retrieval pipeline and evaluated the results on four metrics, viz., Normalized Discounted Cumulative Gain (NDCG), Mean Average Precision (MAP), Recall and Precision .

In our study, we consider the following cases of retrieval:

<img src="plots/pipelines_taxonomy.png" alt="RAG Pipelines Taxonomy" align="middle" width="600" height="300">

The following rankers were used:

- **Diversity Ranker:** The Diversity Ranker enhances the diversity of the paragraphs selected for the context window.

- **Lost In The Middle Ranker:** The Lost In The Middle Ranker optimizes the layout of the selected documents in the LLM’s context window.

- **Transformers Similarity Ranker:** The Transformers Similarity Ranker ranks Documents based on how similar they are to the query. It uses a pre-trained cross-encoder model to embed both the query and the Documents. It then compares the embeddings to determine how similar they are.

Dense Retrieval:

For Dense retrieval, `INSTRUCTOR-XL` and `all-mpnet-base-v2` models were employed.

<img src="plots/rankers_dense_pipeline.png" alt="Dense Pipeline with Rankers" align="middle" width="550" height="100">

Hybrid Retrieval:

BM25 retrieval was used for Sparse retrieval in the Hybrid pipelines. The `bge-reranker-large` model was used in the Similarity Ranker, and `ms-marco-MiniLM-L-12-v2` for the Diversity Ranker.

To combine the results for Hybrid retrieval, Reciprocal Rank Fusion (RRF) was used.

<img src="plots/rankers_hybrid_pipeline.png" alt="Hybrid Pipeline with Rankers" align="middle" width="820" height="230">

## Usage

To run the pipelines, you will need to clone this repository and install the required libraries.

1. Install the `llms_as_rankers` package:

```bash
git clone https://github.com/avnlp/llms-as-rankers
cd llm-as-rankers
pip install -e .
```

2. To add the data to an index in Pinecone using the INSTRUCTOR-XL embedding model:

```python
cd src/llms_as_rankers/indexing_pipeline/fiqa
python pinecone_instructor_index.py
```

3. To run a specific pipeline you will have to go that file path and then run the file.
For example, running the pipeline using dense retrieval with Diversity Ranker, Lost In The Middle Ranker and Similarity Ranker:

```python
cd src/llms_as_rankers/pointwise/instructor_xl/fiqa/
python dense_similarity_diversity_litm.py
```

## License

The source files are distributed under the [MIT License](https://github.com/avnlp/llms-as-rankers/blob/main/LICENSE).
